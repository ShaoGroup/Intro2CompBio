
\documentclass[a4paper]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{float}
\usepackage{CJK}
\usepackage{geometry}
\usepackage{url}
\usepackage{polynom}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{cite}
\usepackage{subfigure}
\usepackage{xcolor}
\usepackage{listings}
\lstset{
   %行号
   numbers=left,
   %背景框
   framexleftmargin=10mm,
   frame=none,
   %背景色
   %backgroundcolor=\color[rgb]{1,1,0.76},
   backgroundcolor=\color[RGB]{245,245,244},
   %样式
   keywordstyle=\bf\color{blue},
   identifierstyle=\bf,
   numberstyle=\color[RGB]{0,192,192},
   commentstyle=\it\color[RGB]{0,96,96},
   stringstyle=\rmfamily\slshape\color[RGB]{128,0,0},
   breaklines,
   tabsize=4,
   %显示空格
   showstringspaces=false
   }


\graphicspath{{fig/}}

\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\setlength{\parindent}{2em}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}
\begin{CJK}{UTF8}{gbsn}

\author{Fei Xia}
\date{\today}
\title{CompBio HW3}
\end{CJK}

\begin{document}
\begin{CJK}{UTF8}{gbsn}
\maketitle

\section{Problem 1}
\subsection{a} % (fold)
For multivariate linear regression model $y=x^T\beta+\epsilon$，$\epsilon \sim $ $N(0,\sigma^2)$, prove MLE and LSE is equivalent.
{\bf LSE} % (fold)
from $y=x^T\beta+\epsilon$，$\epsilon$~$N(0,\sigma^2)$ and $\hat{y}=x^T\beta$

MSE is equal to: 
\begin{equation}
	\sum_i{(y_i-\hat{y}_i)^2}=\sum_i{(y_i-x_i^T\beta)^2}=(Y-X^T\beta)^T(Y-X^T\beta)
\end{equation}
with $Y=(y_1,y_2,...,y_n)^T,X=(x_1,x_2,...,x_n)^T$

according to LSE formulation:

\begin{equation}
    \min_{\beta}{(Y-X^T\beta)^T(Y-X^T\beta)}
\end{equation}

derivative of $\beta$ is $0$，
\begin{equation}
    \left.
    \begin{aligned}
        \frac{\partial (Y-X^T\beta)^T(Y-X^T\beta)}{\partial \beta} = -2X(Y-X^T\beta)\\
		=-2XY+2XX^T\beta=0
    \end{aligned}
    \right.
\end{equation}

we get 

\begin{equation}
    \hat{\beta}=(XX^T)^{-1}XY
\end{equation}

{\bf MLE} % (fold)


because $\epsilon \sim N(0,\sigma^2)$

The likelihood function is:
\begin{equation}
    \left.
    \begin{aligned}
        L(\beta)=\prod_i{P(y_i|x_i,\beta)}\\
		=C_1 e^{C_2\sum_i{(y_i-x_i^T\beta)^2}}\\
		=C_1 e^{-C_2 (Y-X^T\beta)^T(Y-X^T\beta)}\\
    \end{aligned}
    \right.
\end{equation}
with $C_1>0,C_2>0$ being constants.

LL is equal to:
\begin{equation}
    \ln L(\beta)=-C (Y-X^T\beta)^T(Y-X^T\beta)
\end{equation}
with $C>0$ being a constant

gives the optimization formulation of the problem:
\begin{equation}
    \max_{\beta}{-C (Y-X^T\beta)^T(Y-X^T\beta)}
\end{equation}
which is exactly the same as LSE formulation.

\subsection{b} % (fold)
In univariate regression model，for one dimension $x_j$ in $x$，we have $y_j=x_j\beta_j+\epsilon$。

Solve this problem using LSE:
\begin{equation}
    \min_{\beta_j}{\sum_i{(y_{ij}-x_{ij}\beta_j)^2}}
\end{equation}
we get 
\begin{equation}
	\beta^*_j=\frac{\sum_i{x_{ij}y_{ij}}}{\sum_i{x_{ij}^2}}
\end{equation}

We compare this with the results we got above: $\hat{\beta}=(XX^T)^{-1}XY$. 

For the coefficient in multivariate regression $\hat{\beta}$, one dimension $\hat{\beta}_j$ is related to all dimension of data, but for univariate regression, each dimension $\beta^*_j$ is only related to that corresponding dimension of data. 

\end{CJK}
\end{document} 